{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenACC Acceleration \n",
    "Let's execute the cell below to display information about the GPUs running on the server by running the `nvidia-smi` command, which ships with the Nvidia GPU Drivers that we will be using. To do this, execute the cell block below by giving it focus (clicking on it with your mouse), and hitting Ctrl-Enter, or pressing the play button in the toolbar above. If all goes well, you should see some output returned below the grey cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed May 18 10:55:58 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.103.01   Driver Version: 470.103.01   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM...  On   | 00000000:07:00.0 Off |                   On |\n",
      "| N/A   25C    P0    48W / 400W |     20MiB / 81251MiB |     N/A      Default |\n",
      "|                               |                      |              Enabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------+\n",
      "| MIG devices:                                                                |\n",
      "+------------------+----------------------+-----------+-----------------------+\n",
      "| GPU  GI  CI  MIG |         Memory-Usage |        Vol|         Shared        |\n",
      "|      ID  ID  Dev |           BAR1-Usage | SM     Unc| CE  ENC  DEC  OFA  JPG|\n",
      "|                  |                      |        ECC|                       |\n",
      "|==================+======================+===========+=======================|\n",
      "|  0    3   0   0  |      6MiB / 19968MiB | 28      0 |  2   0    1    0    0 |\n",
      "|                  |      0MiB / 32767MiB |           |                       |\n",
      "+------------------+----------------------+-----------+-----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the code will be run on Multicore as well try running the cell below and get details of the nnumber of core and CPU architecure on the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat /proc/cpuinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy and Compile the Serial code\n",
    "\n",
    "Before start modifying the serial code, let's make a copy of the serial code and rename it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp ../source_code/serial/* ../source_code/openacc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f arraymalloc.o boundary.o cfd.o cfdio.o jacobi.o cfd velocity.dat colourmap.dat cfd.plt core\n",
      "nvc++ -lm -I/opt/nvidia/hpc_sdk/Linux_x86_64/21.3/cuda/11.2/include -L/opt/nvidia/hpc_sdk/Linux_x86_64/21.3/cuda/11.2/lib64 -lnvToolsExt -c arraymalloc.cpp\n",
      "nvc++ -lm -I/opt/nvidia/hpc_sdk/Linux_x86_64/21.3/cuda/11.2/include -L/opt/nvidia/hpc_sdk/Linux_x86_64/21.3/cuda/11.2/lib64 -lnvToolsExt -c boundary.cpp\n",
      "nvc++ -lm -I/opt/nvidia/hpc_sdk/Linux_x86_64/21.3/cuda/11.2/include -L/opt/nvidia/hpc_sdk/Linux_x86_64/21.3/cuda/11.2/lib64 -lnvToolsExt -c cfd.cpp\n",
      "nvc++ -lm -I/opt/nvidia/hpc_sdk/Linux_x86_64/21.3/cuda/11.2/include -L/opt/nvidia/hpc_sdk/Linux_x86_64/21.3/cuda/11.2/lib64 -lnvToolsExt -c cfdio.cpp\n",
      "\u001b[01m\u001b[0m\u001b[01m\"cfdio.cpp\", line 20\u001b[0m: \u001b[01;35mwarning\u001b[0m: variable \u001b[01m\"nvel\"\u001b[0m was declared but never referenced\n",
      "    int nvel, nrgb;\n",
      "        ^\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m\"cfdio.cpp\", line 20\u001b[0m: \u001b[01;35mwarning\u001b[0m: variable \u001b[01m\"nrgb\"\u001b[0m was declared but never referenced\n",
      "    int nvel, nrgb;\n",
      "              ^\n",
      "\n",
      "nvc++ -lm -I/opt/nvidia/hpc_sdk/Linux_x86_64/21.3/cuda/11.2/include -L/opt/nvidia/hpc_sdk/Linux_x86_64/21.3/cuda/11.2/lib64 -lnvToolsExt -c jacobi.cpp\n",
      "nvc++ -lm -I/opt/nvidia/hpc_sdk/Linux_x86_64/21.3/cuda/11.2/include -L/opt/nvidia/hpc_sdk/Linux_x86_64/21.3/cuda/11.2/lib64 -lnvToolsExt -o cfd arraymalloc.o boundary.o cfd.o cfdio.o jacobi.o \n"
     ]
    }
   ],
   "source": [
    "!cd ../source_code/openacc && make clean && make"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Serial code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scale Factor = 64, iterations = 500\n",
      "Irrotational flow\n",
      "Running CFD on 2048 x 2048 grid in serial\n",
      "\n",
      "Starting main loop...\n",
      "\n",
      "\n",
      "... finished\n",
      "After 500 iterations, the error is 0.00211211\n",
      "Time for 500 iterations was 7.76763 seconds\n",
      "Each iteration took 0.0155353 seconds\n",
      "\n",
      "\n",
      "Writing data files ...\n",
      "... done!\n",
      "\n",
      "Written gnuplot script 'cfd.plt'\n",
      "... finished\n"
     ]
    }
   ],
   "source": [
    "!cd ../source_code/openacc && ./cfd 64 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Start adding OpenACC Pragmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can start modifying the C++ code and the `Makefile`:\n",
    "\n",
    "[cfd code](../source_code/openacc/cfd.cpp) \n",
    "\n",
    "[Makefile](../source_code/openacc/Makefile)\n",
    "\n",
    "Remember to **SAVE** your code after changes, before running below cells.\n",
    "\n",
    "#### Some Hints\n",
    "\n",
    "1) Notice implicit and explicit copy of variables --> Add `-Minfo=accel` flag to `Makefile`.\n",
    "\n",
    "2) Check if there is any data race in your code.( More details on data race is present in the Links and resources section below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and run OpenACC enabled code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f arraymalloc.o boundary.o cfd.o cfdio.o jacobi.o cfd velocity.dat colourmap.dat cfd.plt core\n",
      "nvc++ -acc -ta=tesla:managed,lineinfo -Minfo=accel -lm -I/opt/nvidia/hpc_sdk/Linux_x86_64/21.3/cuda/11.2/include -L/opt/nvidia/hpc_sdk/Linux_x86_64/21.3/cuda/11.2/lib64 -lnvToolsExt -c arraymalloc.cpp\n",
      "nvc++ -acc -ta=tesla:managed,lineinfo -Minfo=accel -lm -I/opt/nvidia/hpc_sdk/Linux_x86_64/21.3/cuda/11.2/include -L/opt/nvidia/hpc_sdk/Linux_x86_64/21.3/cuda/11.2/lib64 -lnvToolsExt -c boundary.cpp\n",
      "nvc++ -acc -ta=tesla:managed,lineinfo -Minfo=accel -lm -I/opt/nvidia/hpc_sdk/Linux_x86_64/21.3/cuda/11.2/include -L/opt/nvidia/hpc_sdk/Linux_x86_64/21.3/cuda/11.2/lib64 -lnvToolsExt -c cfd.cpp\n",
      "main:\n",
      "    220, Generating Tesla code\n",
      "        220, #pragma acc loop gang, vector(128) collapse(2) /* blockIdx.x threadIdx.x */\n",
      "        222,   /* blockIdx.x threadIdx.x collapsed */\n",
      "    220, Generating implicit copyin(psitmp[:]) [if not already present]\n",
      "         Generating implicit copyout(psi[:]) [if not already present]\n",
      "nvc++ -acc -ta=tesla:managed,lineinfo -Minfo=accel -lm -I/opt/nvidia/hpc_sdk/Linux_x86_64/21.3/cuda/11.2/include -L/opt/nvidia/hpc_sdk/Linux_x86_64/21.3/cuda/11.2/lib64 -lnvToolsExt -c cfdio.cpp\n",
      "\u001b[01m\u001b[0m\u001b[01m\"cfdio.cpp\", line 20\u001b[0m: \u001b[01;35mwarning\u001b[0m: variable \u001b[01m\"nvel\"\u001b[0m was declared but never referenced\n",
      "    int nvel, nrgb;\n",
      "        ^\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m\"cfdio.cpp\", line 20\u001b[0m: \u001b[01;35mwarning\u001b[0m: variable \u001b[01m\"nrgb\"\u001b[0m was declared but never referenced\n",
      "    int nvel, nrgb;\n",
      "              ^\n",
      "\n",
      "nvc++ -acc -ta=tesla:managed,lineinfo -Minfo=accel -lm -I/opt/nvidia/hpc_sdk/Linux_x86_64/21.3/cuda/11.2/include -L/opt/nvidia/hpc_sdk/Linux_x86_64/21.3/cuda/11.2/lib64 -lnvToolsExt -c jacobi.cpp\n",
      "jacobistep(double *, double *, int, int):\n",
      "     11, Generating Tesla code\n",
      "         11, #pragma acc loop gang, vector(128) collapse(2) /* blockIdx.x threadIdx.x */\n",
      "         13,   /* blockIdx.x threadIdx.x collapsed */\n",
      "     11, Generating implicit copyout(psinew[:]) [if not already present]\n",
      "         Generating implicit copyin(psi[:]) [if not already present]\n",
      "jacobistepvort(double *, double *, double *, double *, int, int, double):\n",
      "     33, Generating Tesla code\n",
      "         33, #pragma acc loop gang, vector(128) collapse(2) /* blockIdx.x threadIdx.x */\n",
      "         35,   /* blockIdx.x threadIdx.x collapsed */\n",
      "     33, Generating implicit copyin(psi[:],zet[:]) [if not already present]\n",
      "         Generating implicit copyout(psinew[:]) [if not already present]\n",
      "deltasq(double *, double *, int, int):\n",
      "     65, Generating Tesla code\n",
      "         65, #pragma acc loop gang, vector(128) collapse(2) /* blockIdx.x threadIdx.x */\n",
      "         67,   /* blockIdx.x threadIdx.x collapsed */\n",
      "     65, Generating implicit copyin(oldarr[:],newarr[:]) [if not already present]\n",
      "         Generating implicit copy(dsq) [if not already present]\n",
      "nvc++ -acc -ta=tesla:managed,lineinfo -Minfo=accel -lm -I/opt/nvidia/hpc_sdk/Linux_x86_64/21.3/cuda/11.2/include -L/opt/nvidia/hpc_sdk/Linux_x86_64/21.3/cuda/11.2/lib64 -lnvToolsExt -o cfd arraymalloc.o boundary.o cfd.o cfdio.o jacobi.o \n"
     ]
    }
   ],
   "source": [
    "!cd ../source_code/openacc && make clean && make"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint : Add `-Minfo=accel` to the `Makefile` to check that Kernel code indeed has been generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profile the OpenACC Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "WARNING: The command line includes a target application therefore the CPU context-switch scope has been set to process-tree.\n",
      "Collecting data...\n",
      "Scale Factor = 64, iterations = 500\n",
      "Irrotational flow\n",
      "Running CFD on 2048 x 2048 grid in serial\n",
      "\n",
      "Starting main loop...\n",
      "\n",
      "\n",
      "... finished\n",
      "After 500 iterations, the error is 0.00211211\n",
      "Time for 500 iterations was 0.210083 seconds\n",
      "Each iteration took 0.000420166 seconds\n",
      "\n",
      "\n",
      "Writing data files ...\n",
      "... done!\n",
      "\n",
      "Written gnuplot script 'cfd.plt'\n",
      "... finished\n",
      "Processing events...\n",
      "Saving temporary \"/tmp/nsys-report-6f02-6ab2-c5b7-6da5.qdstrm\" file to disk...\n",
      "\n",
      "Creating final output files...\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-6f02-6ab2-c5b7-6da5.qdrep\"\n",
      "Exporting 15398 events: [=================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-6f02-6ab2-c5b7-6da5.sqlite\n",
      "\n",
      "\n",
      "CUDA API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum           Name        \n",
      " -------  ---------------  ---------  ----------  --------  --------  --------------------\n",
      "    80.6        178585662       1001    178407.3      9730  11975135  cuCtxSynchronize    \n",
      "     9.1         20240031          1  20240031.0  20240031  20240031  cuMemAllocManaged   \n",
      "     7.7         17049128          1  17049128.0  17049128  17049128  cuMemHostAlloc      \n",
      "     1.9          4168444       1001      4164.3      3547     40896  cuLaunchKernel      \n",
      "     0.4           835406          2    417703.0      8106    827300  cuMemAllocHost_v2   \n",
      "     0.2           480295          3    160098.3      7115    450613  cuMemAlloc_v2       \n",
      "     0.0            75558          1     75558.0     75558     75558  cuModuleLoadDataEx  \n",
      "     0.0            36426          2     18213.0     17226     19200  cuStreamCreate      \n",
      "     0.0            31306          1     31306.0     31306     31306  cuMemcpyDtoHAsync_v2\n",
      "     0.0            10001          1     10001.0     10001     10001  cuMemsetD32Async    \n",
      "     0.0             6664          4      1666.0       641      3618  cuEventCreate       \n",
      "     0.0             5742          1      5742.0      5742      5742  cuEventRecord       \n",
      "     0.0             1944          1      1944.0      1944      1944  cuEventSynchronize  \n",
      "\n",
      "\n",
      "\n",
      "CUDA Kernel Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Instances   Average   Minimum  Maximum                       Name                     \n",
      " -------  ---------------  ---------  ---------  -------  --------  ---------------------------------------------\n",
      "    51.2         95010694        500   190021.4   157502  11976680  jacobistep_11_gpu(double*, double*, int, int)\n",
      "    43.8         81370469        500   162740.9   145660    172449  main_220_gpu                                 \n",
      "     5.0          9354894          1  9354894.0  9354894   9354894  deltasq_65_gpu(double*, double*, int, int)   \n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by time):\n",
      "\n",
      " Time(%)  Total Time (ns)  Operations  Average  Minimum  Maximum              Operation            \n",
      " -------  ---------------  ----------  -------  -------  -------  ---------------------------------\n",
      "    65.7          3671850         650   5649.0     3327    54315  [CUDA Unified Memory memcpy HtoD]\n",
      "    34.1          1906675         197   9678.6     2208    51402  [CUDA Unified Memory memcpy DtoH]\n",
      "     0.1             3712           1   3712.0     3712     3712  [CUDA memcpy DtoH]               \n",
      "     0.0             2497           1   2497.0     2497     2497  [CUDA memset]                    \n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by size in KiB):\n",
      "\n",
      "   Total    Operations  Average  Minimum  Maximum               Operation            \n",
      " ---------  ----------  -------  -------  --------  ---------------------------------\n",
      " 32896.000         197  166.985    4.000  1020.000  [CUDA Unified Memory memcpy DtoH]\n",
      " 32896.000         650   50.609    4.000   972.000  [CUDA Unified Memory memcpy HtoD]\n",
      "     0.008           1    0.008    0.008     0.008  [CUDA memcpy DtoH]               \n",
      "     0.008           1    0.008    0.008     0.008  [CUDA memset]                    \n",
      "\n",
      "\n",
      "\n",
      "NVTX Push-Pop Range Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Instances    Average     Minimum    Maximum           Range        \n",
      " -------  ---------------  ---------  -----------  ---------  ---------  ---------------------\n",
      "    48.5        210081615          1  210081615.0  210081615  210081615  Overall_Iteration    \n",
      "    23.8        103132140        500     206264.3     172761   12202801  JacobiStep           \n",
      "    20.6         89265684        500     178531.4     170847     204117  Switch_Array         \n",
      "     4.0         17360343        500      34720.7        130   17229134  Calculate_Error      \n",
      "     1.6          6974937          1    6974937.0    6974937    6974937  Initialization       \n",
      "     1.5          6327093          1    6327093.0    6327093    6327093  Compute_Normalization\n",
      "     0.0            15773          1      15773.0      15773      15773  Boundary_PSI         \n",
      "\n",
      "Report file moved to \"/home/pfbhqx4d/workspace-nways-challenge/C/source_code/openacc/minicfdopenacc_profile.qdrep\"\n",
      "Report file moved to \"/home/pfbhqx4d/workspace-nways-challenge/C/source_code/openacc/minicfdopenacc_profile.sqlite\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cd ../source_code/openacc && nsys profile -t nvtx,openacc,cuda --stats=true --force-overwrite true -o minicfdopenacc_profile ./cfd 64 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "WARNING: The command line includes a target application therefore the CPU context-switch scope has been set to process-tree.\n",
      "Collecting data...\n",
      "Scale Factor = 64, iterations = 500\n",
      "Irrotational flow\n",
      "Running CFD on 2048 x 2048 grid in serial\n",
      "\n",
      "Starting main loop...\n",
      "\n",
      "\n",
      "... finished\n",
      "After 500 iterations, the error is 0.00211211\n",
      "Time for 500 iterations was 0.239742 seconds\n",
      "Each iteration took 0.000479484 seconds\n",
      "\n",
      "\n",
      "Writing data files ...\n",
      "... done!\n",
      "\n",
      "Written gnuplot script 'cfd.plt'\n",
      "... finished\n",
      "Processing events...\n",
      "Saving temporary \"/tmp/nsys-report-910d-e06b-f0ee-5701.qdstrm\" file to disk...\n",
      "\n",
      "Creating final output files...\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-910d-e06b-f0ee-5701.qdrep\"\n",
      "Exporting 15431 events: [=================================================100%]                                         ]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-910d-e06b-f0ee-5701.sqlite\n",
      "\n",
      "\n",
      "CUDA API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum           Name        \n",
      " -------  ---------------  ---------  ----------  --------  --------  --------------------\n",
      "    81.7        204862322       1001    204657.7      3528  11773584  cuCtxSynchronize    \n",
      "     8.3         20909080          1  20909080.0  20909080  20909080  cuMemAllocManaged   \n",
      "     6.9         17298148          1  17298148.0  17298148  17298148  cuMemHostAlloc      \n",
      "     2.5          6268885       1001      6262.6      3488    703060  cuLaunchKernel      \n",
      "     0.3           824094          2    412047.0      7506    816588  cuMemAllocHost_v2   \n",
      "     0.2           475925          3    158641.7      9520    445181  cuMemAlloc_v2       \n",
      "     0.0            72442          1     72442.0     72442     72442  cuModuleLoadDataEx  \n",
      "     0.0            37879          2     18939.5     18258     19621  cuStreamCreate      \n",
      "     0.0            33089          1     33089.0     33089     33089  cuMemcpyDtoHAsync_v2\n",
      "     0.0            10923          1     10923.0     10923     10923  cuMemsetD32Async    \n",
      "     0.0             5973          1      5973.0      5973      5973  cuEventRecord       \n",
      "     0.0             5591          4      1397.8       551      3106  cuEventCreate       \n",
      "     0.0             2024          1      2024.0      2024      2024  cuEventSynchronize  \n",
      "\n",
      "\n",
      "\n",
      "CUDA Kernel Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Instances   Average    Minimum   Maximum                       Name                     \n",
      " -------  ---------------  ---------  ----------  --------  --------  ---------------------------------------------\n",
      "    50.5         94469887        500    188939.8    156798  11775298  jacobistep_11_gpu(double*, double*, int, int)\n",
      "    43.4         81236122        500    162472.2    147996    174625  main_220_gpu                                 \n",
      "     6.1         11500556          1  11500556.0  11500556  11500556  deltasq_65_gpu(double*, double*, int, int)   \n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by time):\n",
      "\n",
      " Time(%)  Total Time (ns)  Operations  Average  Minimum  Maximum              Operation            \n",
      " -------  ---------------  ----------  -------  -------  -------  ---------------------------------\n",
      "    66.7          3837871         673   5702.6     3327    54666  [CUDA Unified Memory memcpy HtoD]\n",
      "    33.2          1910415         197   9697.5     2175    51338  [CUDA Unified Memory memcpy DtoH]\n",
      "     0.1             5025           1   5025.0     5025     5025  [CUDA memcpy DtoH]               \n",
      "     0.0             2785           1   2785.0     2785     2785  [CUDA memset]                    \n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by size in KiB):\n",
      "\n",
      "   Total    Operations  Average  Minimum  Maximum               Operation            \n",
      " ---------  ----------  -------  -------  --------  ---------------------------------\n",
      " 32896.000         197  166.985    4.000  1020.000  [CUDA Unified Memory memcpy DtoH]\n",
      " 32896.000         673   48.880    4.000   964.000  [CUDA Unified Memory memcpy HtoD]\n",
      "     0.008           1    0.008    0.008     0.008  [CUDA memcpy DtoH]               \n",
      "     0.008           1    0.008    0.008     0.008  [CUDA memset]                    \n",
      "\n",
      "\n",
      "\n",
      "NVTX Push-Pop Range Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Instances    Average     Minimum    Maximum           Range        \n",
      " -------  ---------------  ---------  -----------  ---------  ---------  ---------------------\n",
      "    48.7        239740756          1  239740756.0  239740756  239740756  Overall_Iteration    \n",
      "    24.0        118410533        500     236821.1     171990   11978933  JacobiStep           \n",
      "    21.0        103289057        500     206578.1     169063     903821  Switch_Array         \n",
      "     3.6         17615403        500      35230.8        140   17484960  Calculate_Error      \n",
      "     1.4          6964666          1    6964666.0    6964666    6964666  Initialization       \n",
      "     1.3          6353959          1    6353959.0    6353959    6353959  Compute_Normalization\n",
      "     0.0            18098          1      18098.0      18098      18098  Boundary_PSI         \n",
      "\n",
      "Report file moved to \"/home/pfbhqx4d/workspace-nways-challenge/C/source_code/openacc/minicfdopenacc_profile.qdrep\"\n",
      "Report file moved to \"/home/pfbhqx4d/workspace-nways-challenge/C/source_code/openacc/minicfdopenacc_profile.sqlite\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# multicore version\n",
    "!cd ../source_code/openacc && nsys profile -t nvtx,openacc,cuda --stats=true --force-overwrite true -o minicfdopenacc_profile ./cfd 64 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can examine the output on the terminal or you can download the file and view the timeline by opening the output with the NVIDIA Nsight Systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "Download and save the profiler report file by holding down <mark>Shift</mark> and <mark>Right-Clicking</mark> [Here](../source_code/openacc/minicfdopenacc_profile.qdrep).\n",
    "\n",
    "## Validating the Output\n",
    "\n",
    "Make sure the error value printed as output matches that of the serial code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendations for adding OpenACC Pragmas\n",
    "\n",
    "After finding the hotspot function take an incremental approach to add pargmas. \n",
    "\n",
    "1) Ignore the initialization, finalization and I/O functions\n",
    "\n",
    "2) Take an incremental approach by adding pragmas one at a time\n",
    "\n",
    "3) Unified Memory provides a good start point where you need not worry about the data transfers (`–ta=tesla:managed`)\n",
    "\n",
    "4) Cross check the output after incremental changes to check algorithmic scalability\n",
    "\n",
    "5) Move on to using data clauses for better performance \n",
    "\n",
    "6) Start with a small problem size that reduces the execution time. \n",
    "\n",
    "\n",
    "**General tip:** Be aware of *Data Race* situation in which at least two threads access a shared variable at the same time. At least on thread tries to modify the variable. If data race happened, an incorrect result will be returned. So, make sure to validate your output against the serial version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Links and Resources\n",
    "\n",
    "[OpenACC API Guide](https://www.openacc.org/sites/default/files/inline-files/OpenACC%20API%202.6%20Reference%20Guide.pdf)\n",
    "\n",
    "[NVIDIA Nsight System](https://docs.nvidia.com/nsight-systems/)\n",
    "\n",
    "[CUDA Toolkit Download](https://developer.nvidia.com/cuda-downloads)\n",
    "\n",
    "**NOTE**: To be able to see the Nsight System profiler output, please download Nsight System latest version from [here](https://developer.nvidia.com/nsight-systems).\n",
    "\n",
    "Don't forget to check out additional [OpenACC Resources](https://www.openacc.org/resources) and join our [OpenACC Slack Channel](https://www.openacc.org/community#slack) to share your experience and get more help from the community.\n",
    "\n",
    "--- \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Licensing \n",
    "\n",
    "This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
